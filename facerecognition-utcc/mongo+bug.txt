import cv2
import face_recognition
import numpy as np
from pymongo import MongoClient

# Connect to MongoDB
client = MongoClient('mongodb://localhost:27017')
db = client['faces']
collection = db['faces']

# Store photo to MongoDB
photo_path = r'C:\Users\magesy\Desktop\projects\face recognition\faces\rittikiat.png'
# Read the photo file
with open(photo_path, 'rb') as file:
    photo_data = file.read()

# Create a document to insert into the collection
document = {"photo": photo_data}

# Insert the document into the collection
collection.insert_one(document)
print("Photo inserted into MongoDB.")

# Retrieve the document from the collection
document = collection.find_one()

# Access the photo data from the document
photo_data = document["photo"]

# Decode the photo data to an image
nparr = np.frombuffer(photo_data, np.uint8)
photo = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

# Convert the image from BGR to RGB
rgb_photo = cv2.cvtColor(photo, cv2.COLOR_BGR2RGB)

# Detect the face location in the photo using the cnn model
face_location = face_recognition.face_locations(rgb_photo, model="cnn")[0]

print("Face location detected:", face_location)

# Load known faces from MongoDB
known_faces = []
known_names = []
for document in collection.find():
    known_faces.append(face_recognition.face_encodings(document['photo'])[0])  # Use 'photo' instead of 'image'
    known_names.append(document['name'])


# Initialize video capture
video_capture = cv2.VideoCapture(0)

while True:
    # Capture frame-by-frame
    ret, frame = video_capture.read()

    # Resize frame to speed up face recognition
    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)

    # Convert the image from BGR color to RGB
    rgb_frame = small_frame[:, :, ::-1]

    # Find all the faces and their encodings in the current frame
    face_locations = face_recognition.face_locations(rgb_frame)
    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)

    face_names = []
    face_confidences = []

    for face_encoding in face_encodings:
        # Compare face encoding with known faces
        matches = face_recognition.compare_faces(known_faces, face_encoding)
        name = "Unknown"
        confidence = 0

        if True in matches:
            # Find indexes of all matched faces
            matched_indexes = [i for (i, b) in enumerate(matches) if b]
            face_distances = face_recognition.face_distance(known_faces, face_encoding)

            # Find the best match
            best_match_index = np.argmin(face_distances)
            if matches[best_match_index]:
                name = known_names[best_match_index]
                confidence = 1 - face_distances[best_match_index]

        face_names.append(name)
        face_confidences.append(confidence)

    # Draw rectangles and labels on the faces
    for (top, right, bottom, left), name, confidence in zip(face_locations, face_names, face_confidences):
        # Scale back up the face locations
        top *= 4
        right *= 4
        bottom *= 4
        left *= 4

        # Draw a rectangle around the face
        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)

        # Draw the label and confidence level below the face
        label = f"{name}: {round(confidence * 100, 2)}%"
        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)
        cv2.putText(frame, label, (left + 6, bottom - 6), cv2.FONT_HERSHEY_DUPLEX, 0.8, (255, 255, 255), 1)

    # Display the resulting image
    cv2.imshow('Face Recognition', frame)

    # Break the loop when 'q' is pressed
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# Release the video capture object and close the window
video_capture.release()
cv2.destroyAllWindows()
